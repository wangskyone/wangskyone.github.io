<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
"http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Fei Zhu</title>
</head>
<body style="width: 1100px; margin: 0 auto;">
<div id="fwtitle">
<div id="toptitle">
<h1>Fei Zhu</h1>
</div>
</div>
<div id="layout-content">
<table class="imgtable"><tr><td>
<img src="zhufei.jpg" alt="photo_me" width="130px" height="180px" />&nbsp;</td>
<td align="left"><p>Assistant Professor [<a href="https://scholar.google.com/citations?user=fjZ1CBwAAAAJ&hl=zh-CN">Google Scholar</a>]<br />
<div style="text-align: left;">
  Centre for Artificial Intelligence and Robotics, Hong Kong Institute of Science & Innovation<br>
  Chinese Academy of Sciences<br>
  3/F, 17W, Science Park West Avenue, Hong Kong Science Park, Hong Kong <br>
  <br />
  Email: zhfei2018@gmail.com<br>
  WeChat: 17888841931
</div>

</p>
</td></tr></table>
<h2>About Me</h2>
<p>I am currently an Assistant Professor at the Centre for Artificial Intelligence and Robotics, Hong Kong Institute of Science & Innovation, Chinese Academy of Sciences, working with Prof. <a href="https://zhaoxiangzhang.net/">Zhaoxiang Zhang</a> and Prof. Gaofeng Meng. I received my Ph.D. in Pattern Recognition and Intelligent Systems from
the Institute of Automation, Chinese Academy of Sciences, where I was advised by Prof. <a href="http://www.nlpr.ia.ac.cn/liucl/">Cheng-Lin Liu</a> and Prof. <a href="https://people.ucas.edu.cn/~xuyaozhang">Xu-Yao Zhang</a>. Prior to this, I received the B.E. degree from Tsinghua University.
</p>
<p>
<span style="font-weight: bold; color: purple;">Research Highlights</span>: My research focuses on both the theoretical and applied aspects of dynamic learning, especially for foundation models such as MLLMs and LLMs. Additionally, I am keen on utilizing these models to facilitate applications in biomedicine and embodied robotics.¬†
</p>
<p>
<span style="font-weight: bold; color: purple;">Related ML topics</span>: continual pre-training, continual post-training, reinforcement fine-tuning, AI alignment. <br />
<span style="font-weight: bold; color: purple;">Focused applications</span>: biomedicine and healthcare, robot learning and embodied AI.¬† <br />
</p>
<p style="text - decoration: underline purple; font - weight: bold; color: purple;">
We are looking for collaborators, who are self-motivated and have a solid foundation in mathematics and programming. If you are interested, please contact me via email (zhfei2018@gmail.com) or WeChat (17888841931). <br />
</p>
<!--
<h2>News & Updates </h2> 
<ul>

<div style="height:80px;width:fit-content;overflow:auto;background:#FFFFFF;">
	<li>
		<p>[2023/02/28] One paper on <b>misclassification detection</b> is accepted by <b>CVPR</b> 2023, code is coming soon.
			</p>
	</li>
</div>
</ul>
-->

</p>

<h2>Invited Talks</h2> 
<ul>
<li><p> Unknown Rejection in Open Environment, Biomedical Engineering Distinguished Lecture Series, ÂçóÊñπÁßëÊäÄÂ§ßÂ≠¶, August, 2024 </p></li>
<li><p> Deep Continual Learning, School of Computer Science and Engineering, Âçó‰∫¨ÁêÜÂ∑•Â§ßÂ≠¶, January, 2025  </p></li>
<li><p> Open-Environment Continual Learning, ‰∏≠ÂÖ≥Êùë‰∫∫Â∑•Êô∫ËÉΩÁ†îÁ©∂Èô¢, Âåó‰∫¨, February, 2025  </p></li>
<li><p> Continual Learning in Multimodal Large Language Model, VALSE 2025 ÊåÅÁª≠Â≠¶‰π†ËÆ∫Âùõ, Áè†Êµ∑, June, 2025  </p></li>
<li><p> Continual Learning: Theory, Methods and Applications, 2025 ‰∏≠ÂõΩÂõæË±°ÂõæÂΩ¢Â≠¶Â≠¶‰ºöÈùíÂπ¥ÁßëÂ≠¶ÂÆ∂‰ºöËÆÆ, ÈùíÂ≤õ, September, 2025  </p></li>
<li><p> Recent Advance of Continual Learning, Âåó‰∫¨Â§ßÂ≠¶Ê∑±Âú≥Á†îÁ©∂ÁîüÈô¢, October, 2025  </p></li>
</ul>
	
<h2>Selected Publications </h2>
<ul>
<li><p>[NeurIPS 2025 <font color="purple">Spotlight Paper</font>] <i>RobustMerge: Parameter-Efficient Model Merging for MLLMs with Direction Robustness</i> [<a href="https://arxiv.org/pdf/2502.17159">paper</a>].<br />
Fanhu Zeng, Haiyang Guo, <b>Fei Zhu</b><sup>üìß</sup>, Li Shen, Hao Tang. <br /></p>
</li>
</ul>
<ul>
<li><p>[NeurIPS 2025] <i>C-NAV: Towards Self-Evolving Continual Object Navigation in Open World</i> [<a href="https://bigtree765.github.io/C-Nav-project">paper</a>].<br />
Mingming Yu, <b>Fei Zhu</b>, Wenzhuo Liu, Yirong Yang, Yunbo Wang, Wenjun Wu, Jing Liu. <br /></p>
</li>
</ul>
<ul>
<li><p>[TPAMI 2025] <i>PASS++: A Dual Bias Reduction Framework for Non-Exemplar Class-Incremental Learning</i> [<a href="https://arxiv.org/pdf/2407.14029">paper</a>].<br />
<b>Fei Zhu</b>, Xu-Yao Zhang, Zhen Cheng, Cheng-Lin Liu. <br /></p>
</li>
</ul>
<ul>
<li><p>[TPAMI 2024] <i>Revisiting Confidence Estimation: Towards Reliable Failure Prediction</i> [<a href="https://ieeexplore.ieee.org/abstract/document/10356834">paper</a>] [<a href="https://arxiv.org/pdf/2403.02886.pdf">arxiv</a>] [<a href="https://github.com/Impression2805/FMFP">code</a>].<br />
<b>Fei Zhu</b>, Xu-Yao Zhang, Zhen Cheng, Cheng-Lin Liu. <br /></p>
</li>
</ul>
<ul>
<li><p>[TPAMI 2023] <i>Learning by Seeing More Classes</i> [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9964413">paper</a>].<br />
<b>Fei Zhu</b>, Xu-Yao Zhang, Rui-Qi Wang, Cheng-Lin Liu. <br /></p>
</li>
</ul>
<ul>
<li><p>[Neural Networks 2023] <i>Imitating the Oracle: Towards Calibrated Model for Class Incremental Learning</i> [<a href="https://www.sciencedirect.com/science/article/abs/pii/S0893608023001879">paper</a>] [<a href="https://github.com/Impression2805/ItO4CIL">code</a>].<br />
<b>Fei Zhu</b>, Zhen Cheng, Xu-Yao Zhang, Cheng-Lin Liu. <br /></p>
</li>
</ul>
<ul>
<li><p>[CVPR 2024] <i>RCL: Reliable Continual Learning for Unified Failure Detection</i> [<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhu_RCL_Reliable_Continual_Learning_for_Unified_Failure_Detection_CVPR_2024_paper.pdf">paper</a>] [<a href="https://github.com/Impression2805/RCL">code</a>].<br />
<b>Fei Zhu</b>, Zhen Cheng, Xu-Yao Zhang, Cheng-Lin Liu, Zhaoxiang Zhang. <br /></p>
</li>
</ul>
<ul>
<li><p>[CVPR 2023 <font color="purple">Highlight Paper</font> (Top 2.5%)] <i>OpenMix: Exploring Outlier Samples for Misclassification Detection</i> [<a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_OpenMix_Exploring_Outlier_Samples_for_Misclassification_Detection_CVPR_2023_paper.pdf">paper</a>] [<a href="https://arxiv.org/pdf/2303.17093.pdf">arxiv</a>] [<a href="https://github.com/Impression2805/OpenMix">code</a>].<br />
<b>Fei Zhu</b>, Zhen Cheng, Xu-Yao Zhang, Cheng-Lin Liu. <br /></p>
</li>
</ul>
<ul>
<li><p>[ECCV 2022] <i>Rethinking Confidence Calibration for Failure Prediction</i> [<a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136850512.pdf">paper</a>] [<a href="https://arxiv.org/pdf/2303.02970v1.pdf">arxiv</a>] [<a href="https://github.com/Impression2805/FMFP">code</a>].<br />
<b>Fei Zhu</b>, Zhen Cheng, Xu-Yao Zhang, Cheng-Lin Liu. <br /></p>
</li>
</ul>
<ul>
<li><p>[CVPR 2021 <font color="purple">Oral Paper</font> (Top 4%)] <i>Prototype Augmentation and Self-Supervision for Incremental Learning</i> [<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Zhu_Prototype_Augmentation_and_Self-Supervision_for_Incremental_Learning_CVPR_2021_paper.pdf">paper</a>] [<a href="https://github.com/Impression2805/CVPR21_PASS">code</a>].<br />
<b>Fei Zhu</b>, Xu-Yao Zhang, Chuang Wang, Fei Yin, Cheng-Lin Liu. <br /></p>
</li>
</ul>
<ul>
<li><p>[NeurIPS 2021] <i>Class-Incremental Learning via Dual Augmentation</i> [<a href="https://proceedings.neurips.cc/paper/2021/file/77ee3bc58ce560b86c2b59363281e914-Paper.pdf">paper</a>] [<a href="https://github.com/Impression2805/IL2A">code</a>]. <br />
<b>Fei Zhu</b>, Zhen Cheng, Xu-Yao Zhang, Cheng-Lin Liu. <br /></p>
</li>
</ul>
<ul>
<li><p>[IEEE/CAA JAS 2023 <font color="purple">Invited Reviews</font>] <i>Class Incremental Learning: A Review and Performance Evaluation (In Chinese)</i> [<a href="http://www.aas.net.cn/cn/article/doi/10.16383/j.aas.c220588?viewType=HTML">paper</a>].<br />
<b>Fei Zhu</b>, Xu-Yao Zhang, Cheng-Lin Liu. <br /></p>
</li>
</ul>
<ul>
<li><p>[TPAMI 2025] <i>ProtoGCD: Unified and Unbiased Prototype Learning for Generalized Category Discovery</i> [<a href="https://ieeexplore.ieee.org/abstract/document/10948388">paper</a>] [<a href="https://github.com/mashijie1028/ProtoGCD">code</a>].<br />
Shijie Ma, <b>Fei Zhu</b>, Xu-Yao Zhang, Cheng-Lin Liu. <br /></p>
</li>
</ul>
<ul>
<li><p><i>[ICLR 2025] C-CLIP: Multimodal Continual Learning for Vision-Language Model</i> [<a href="https://openreview.net/forum?id=sb7qHFYwBc">paper</a>].<br />
Wen-Zhuo Liu, <b>Fei Zhu</b><sup>üìß</sup>, Longhui Wei, Qi Tian. <br /></p>
</li>
</ul>

<ul>
<li><p><i>[ICCV 2025] Federated Continual Instruction Tuning </i> [<a href="https://arxiv.org/pdf/2503.12897">paper</a>].<br />
Haiyang Guo, Fanhu Zeng, <b>Fei Zhu</b>, Wenzhuo Liu, Da-Han Wang, Jian Xu, Xu-Yao Zhang, Cheng-Lin Liu. <br /></p>
</li>
</ul>
	
<ul>
<li><p><i>[ACL 2025] HiDe-LLaVA: Hierarchical decoupling for continual instruction tuning of multimodal large language model </i> [<a href="https://arxiv.org/abs/2503.12941">paper</a>] [<a href="https://github.com/Ghy0501/HiDe-LLaVA">code</a>].<br />
Haiyang Guo, Fanhu Zeng, Ziwei Xiang, <b>Fei Zhu</b>, Da-Han Wang, Xu-Yao Zhang, Cheng-Lin Liu. <br /></p>
</li>
</ul>
	
<ul>
<li><p><i>[TNNLS 2025] Branch-Tuning: Balancing Stability and Plasticity for Continual Self-Supervised Learning</i> [<a href="https://arxiv.org/pdf/2403.18266">paper</a>].<br />
Wenzhuo Liu, <b>Fei Zhu</b>, Cheng-Lin Liu. <br /></p>
</li>
</ul>
<ul>
<li><p><i>[TNNLS 2025] Average of Pruning: Improving Performance and Stability of Out-of-Distribution Detection</i> [<a href="https://arxiv.org/abs/2303.01201">paper</a>].<br />
Zhen Cheng, <b>Fei Zhu</b>, Xu-Yao Zhang, Cheng-Lin Liu. <br /></p>
</li>
</ul>
<ul>
<li><p>[NeurIPS 2024] <i>MSPE: Multi-Scale Patch Embedding Prompts Vision Transformers to Any Resolution</i> [<a href="https://arxiv.org/pdf/2405.18240">paper</a>].<br />
Wenzhuo Liu, <b>Fei Zhu</b>, Shijie Ma, Cheng-Lin Liu. <br /></p>
</li>
</ul>
<ul>
<li><p>[NeurIPS 2024] <i>Happy: A Debiased Learning Framework for Continual Generalized Category Discovery</i> [<a href="https://proceedings.neurips.cc/paper/2021/file/77ee3bc58ce560b86c2b59363281e914-Paper.pdf">paper</a>] [<a href="https://github.com/mashijie1028/Happy-CGCD">code</a>]. <br />
Shijie Ma, <b>Fei Zhu</b>, Zhun Zhong, Xu-Yao Zhang, Cheng-Lin Liu. <br /></p>
</li>
</ul>
<ul>
<li><p>[IJCV 2024] <i>Breaking the Limits of Reliable Prediction via Generated Data</i> [<a href="https://link.springer.com/article/10.1007/s11263-024-02221-5">paper</a>].<br />
Zhen Cheng, <b>Fei Zhu</b>, Xu-Yao Zhang, Cheng-Lin Liu. <br /></p>
</li>
</ul>
<ul>
<li><p>[PR 2024] <i>Towards trustworthy dataset distillation</i> [<a href="https://www.sciencedirect.com/science/article/pii/S0031320324006265">paper</a>] [<a href="https://github.com/mashijie1028/TrustDD/">code</a>].<br />
Shijie Ma, <b>Fei Zhu</b>, Zhen Cheng, Xu-Yao Zhang. <br /></p>
</li>
</ul>
<ul>
<li><p>[ECCV 2024] <i>PILoRA: Prototype Guided Incremental LoRA for Federated Class-Incremental Learning</i> [<a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/08185.pdf">paper</a>] [<a href="https://arxiv.org/abs/2401.02094">arxiv</a>] [<a href="https://github.com/Ghy0501/PILoRA">code</a>].<br />
Haiyang Guo, <b>Fei Zhu</b>, Wenzhuo Liu, Xu-Yao Zhang, Cheng-Lin Liu. <br /></p>
</li>
</ul>
<ul>
<li><p>[CVPR 2024] <i>Active Generalized Category Discovery</i> [<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Ma_Active_Generalized_Category_Discovery_CVPR_2024_paper.pdf">paper</a>] [<a href="https://arxiv.org/abs/2403.04272">arxiv</a>] [<a href="https://github.com/mashijie1028/ActiveGCD">code</a>].<br />
Shijie Ma, <b>Fei Zhu</b>, Zhun Zhong, Xu-Yao Zhang, Cheng-Lin Liu. <br /></p>
</li>
</ul>
<ul>
<li><p>[PR 2023] <i>Adversarial Training with Distribution Normalization and Margin Balance</i> [<a href="https://www.sciencedirect.com/science/article/pii/S0031320322006616">paper</a>].<br />
Zhen Cheng, <b>Fei Zhu</b>, Xu-Yao Zhang, Cheng-Lin Liu. <br /></p>
</li>
</ul>
<ul>
<li><p>[Nature Communications 2022 <font color="purple">Highlight Paper</font>] <i>Decoding lip language using triboelectric sensors with deep learning</i> [<a href="https://www.nature.com/articles/s41467-022-29083-0">paper</a>].<br />
Yi-Jia Lu*, Han Tan*, Jia Cheng*, <b>Fei Zhu</b>, Bing Liu, Shan-Shan Wei, LinHong Ji, Zhong-Lin Wang. <br /></p>
</li>
</ul>

<h2>Academic Services</h2> 
<ul>
<li><p> Conference Reviewer: NeurIPS, ICLR, ICML, CVPR, ICCV, ECCV </p></li>
<li><p> Journal Reviewer: IEEE TIP, TNNLS, TMM, TKDE, PR, NN, IJCV </p></li>
<li><p> Workshop Organizer: Trustworthy Model and Learning in Open Environment, PRCV 2024 </p></li>
</ul>

</div>
</body>
</html>
